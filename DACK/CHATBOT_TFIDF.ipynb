{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JARVIS LAWYER  APPRENTICE BOT- TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Required Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "import pickle\n",
    "import string\n",
    "import random\n",
    "import timeit\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation - Convert CSV to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/HK5(2024-2025)/CS336 - Truy van thong tin da phuong tien/TF-IDF-DOCUMENT-RETRIEVAL-CHATBOT-master/TF-IDF-DOCUMENT-RETRIEVAL-CHATBOT-master/legal_help_augmented.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/HK5(2024-2025)/CS336 - Truy van thong tin da phuong tien/TF-IDF-DOCUMENT-RETRIEVAL-CHATBOT-master/TF-IDF-DOCUMENT-RETRIEVAL-CHATBOT-master/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# convdata = pd.read_csv(path+'legal_help_clean.csv')\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m convdata_aug \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlegal_help_augmented.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m convdata_aug\u001b[38;5;241m.\u001b[39mhead()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#show header of the dataset\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# convdata.head()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/HK5(2024-2025)/CS336 - Truy van thong tin da phuong tien/TF-IDF-DOCUMENT-RETRIEVAL-CHATBOT-master/TF-IDF-DOCUMENT-RETRIEVAL-CHATBOT-master/legal_help_augmented.csv'"
     ]
    }
   ],
   "source": [
    "# path = '/home/machine1/SANDY/text_mining/CHATBOT/1_TFIDF/'\n",
    "path = 'D:/HK5(2024-2025)/CS336 - Truy van thong tin da phuong tien/TF-IDF-DOCUMENT-RETRIEVAL-CHATBOT-master/TF-IDF-DOCUMENT-RETRIEVAL-CHATBOT-master/'\n",
    "# convdata = pd.read_csv(path+'legal_help_clean.csv')\n",
    "convdata_aug = pd.read_csv(path+'legal_help_augmented.csv')\n",
    "convdata_aug.head()\n",
    "#show header of the dataset\n",
    "# convdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'MESSAGE': \"What are the requirements for me to adopt my nephew whom I have been caring like my own for 7 years after parents' divorce?\",\n",
       "  'RESPONSE': 'You may wish to visit this site: https://app.adoption.gov.sg/AdoptionProcess.aspx'},\n",
       " {'MESSAGE': 'how to have a properly adopted in accordance with the laws of Singapore for my godson,who is a PR now',\n",
       "  'RESPONSE': 'You may want to refer to this site:https://app.msf.gov.sg/Adoption/How-to-adopt-a-citizen-or-PR'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#covert dataframes to json\n",
    "convdata_json = json.loads(convdata.to_json(orient='records'))\n",
    "convdata_json[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as data as JSON\n",
    "with open(path+'conversation_json.json', 'w') as outfile:\n",
    "    json.dump(convdata_json, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#greeting function\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"hello i need help\", \"good day\",\"hey\",\"i need help\", \"can you help me\", \"start\")\n",
    "GREETING_RESPONSES = [\"Good day, How may i of help?\", \"Hello, How can i help you?\", \"Hello\", \"I am glad! You are talking to me.\"]\n",
    "           \n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordnet Lemmatization \n",
    "\n",
    "lemmer = nltk.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "def RemovePunction(tokens):\n",
    "    return[t for t in tokens if t not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('universal_tagset')\n",
    "# Create a stopword list from the standard list of stopwords available in nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Talk_To_Javris(test_set_sentence):\n",
    "    json_file_path = path+\"conversation_json.json\" \n",
    "    tfidf_vectorizer_pickle_path = path + \"tfidf_vectorizer.pkl\"\n",
    "    tfidf_matrix_pickle_path = path+ \"tfidf_matrix_train.pkl\"\n",
    "    \n",
    "    i = 0\n",
    "    sentences = []\n",
    "    \n",
    "    # ---------------Tokenisation of user input -----------------------------#\n",
    "    \n",
    "    tokens = RemovePunction(nltk.word_tokenize(test_set_sentence))\n",
    "    pos_tokens = [word for word,pos in pos_tag(tokens, tagset='universal')]\n",
    "    \n",
    "    word_tokens = LemTokens(pos_tokens)\n",
    "    \n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)  \n",
    "    \n",
    "    filtered_sentence =\" \".join(filtered_sentence).lower()\n",
    "            \n",
    "    test_set = (filtered_sentence, \"\")\n",
    "    \n",
    "    #For Tracing, comment to remove from print.\n",
    "    #print('USER INPUT:'+filtered_sentence)\n",
    "    \n",
    "    # -----------------------------------------------------------------------#\n",
    "        \n",
    "    try: \n",
    "        # ---------------Use Pre-Train Model------------------#\n",
    "        f = open(tfidf_vectorizer_pickle_path, 'rb')\n",
    "        tfidf_vectorizer = pickle.load(f)\n",
    "        f.close()\n",
    "        \n",
    "        f = open(tfidf_matrix_pickle_path, 'rb')\n",
    "        tfidf_matrix_train = pickle.load(f)\n",
    "        # ---------------------------------------------------#\n",
    "    except: \n",
    "        # ---------------To Train------------------#\n",
    "        \n",
    "        start = timeit.default_timer()\n",
    "        \n",
    "        with open(json_file_path) as sentences_file:\n",
    "            reader = json.load(sentences_file)\n",
    "            \n",
    "            # ---------------Tokenisation of training input -----------------------------#    \n",
    "            \n",
    "            for row in reader:\n",
    "                db_tokens = RemovePunction(nltk.word_tokenize(row['MESSAGE']))\n",
    "                pos_db_tokens = [word for word,pos in pos_tag(db_tokens, tagset='universal')]\n",
    "                db_word_tokens = LemTokens(pos_db_tokens)\n",
    "                \n",
    "                db_filtered_sentence = [] \n",
    "                for dbw in db_word_tokens: \n",
    "                    if dbw not in stop_words: \n",
    "                        db_filtered_sentence.append(dbw)  \n",
    "                \n",
    "                db_filtered_sentence =\" \".join(db_filtered_sentence).lower()\n",
    "                \n",
    "                #Debugging Checkpoint\n",
    "                print('TRAINING INPUT: '+db_filtered_sentence)\n",
    "                \n",
    "                sentences.append(db_filtered_sentence)\n",
    "                i +=1                \n",
    "            # ---------------------------------------------------------------------------#\n",
    "                \n",
    "        tfidf_vectorizer = TfidfVectorizer() \n",
    "        tfidf_matrix_train = tfidf_vectorizer.fit_transform(sentences)\n",
    "        \n",
    "        #train timing\n",
    "        stop = timeit.default_timer()\n",
    "        print (\"Training Time : \")\n",
    "        print (stop - start) \n",
    "    \n",
    "        f = open(tfidf_vectorizer_pickle_path, 'wb')\n",
    "        pickle.dump(tfidf_vectorizer, f) \n",
    "        f.close()\n",
    "    \n",
    "        f = open(tfidf_matrix_pickle_path, 'wb')\n",
    "        pickle.dump(tfidf_matrix_train, f) \n",
    "        f.close \n",
    "        # ------------------------------------------#\n",
    "        \n",
    "    #use the learnt dimension space to run TF-IDF on the query\n",
    "    tfidf_matrix_test = tfidf_vectorizer.transform(test_set)\n",
    "\n",
    "    #then run cosine similarity between the 2 tf-idfs\n",
    "    cosine = cosine_similarity(tfidf_matrix_test, tfidf_matrix_train)\n",
    "    \n",
    "    #if not in the topic trained.no similarity \n",
    "    idx= cosine.argsort()[0][-2]\n",
    "    flat =  cosine.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    \n",
    "    if (req_tfidf==0): #Threshold A\n",
    "        \n",
    "        not_understood = \"Apology, I do not understand. Can you rephrase?\"\n",
    "        \n",
    "        return not_understood, not_understood, 2\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        cosine = np.delete(cosine, 0)\n",
    "\n",
    "        #get the max score\n",
    "        max = cosine.max()\n",
    "        response_index = 0\n",
    "\n",
    "        #if max score is lower than < 0.34 > (we see can ask if need to rephrase.)\n",
    "        if (max <= 0.34): #Threshold B\n",
    "            \n",
    "            not_understood = \"Apology, I do not understand. Can you rephrase?\"\n",
    "            \n",
    "            return not_understood,not_understood, 2\n",
    "        else:\n",
    "\n",
    "                #if score is more than 0.91 list the multi response and get a random reply\n",
    "                if (max > 0.91): #Threshold C\n",
    "                    \n",
    "                    new_max = max - 0.05 \n",
    "                    # load them to a list\n",
    "                    list = np.where(cosine > new_max) \n",
    "                   \n",
    "                    # choose a random one to return to the user \n",
    "                    response_index = random.choice(list[0])\n",
    "                else:\n",
    "                    # else we would simply return the highest score\n",
    "                    response_index = np.where(cosine == max)[0][0] + 2 \n",
    "\n",
    "                j = 0 \n",
    "\n",
    "                with open(json_file_path, \"r\") as sentences_file:\n",
    "                    reader = json.load(sentences_file)\n",
    "                    for row in reader:\n",
    "                        j += 1 \n",
    "                        if j == response_index: \n",
    "                            return row[\"RESPONSE\"], row[\"MESSAGE\"], max\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................................................\n",
      "\u001b[1;37;40mJarvis\u001b[0m: My name is Jarvis, a Lawyer Apprentice Bot.\n",
      "\u001b[1;37;40mJarvis\u001b[0m: I will try my best to answer your query.\n",
      "\u001b[1;37;40mJarvis\u001b[0m: If you want to exit, you can type < bye >.\n",
      "......................................................................................\n",
      "......................................................................................\n",
      "\u001b[1;37;40mJARVIS\u001b[0m: Hello\n",
      "......................................................................................\n",
      "......................................................................................\n",
      "\u001b[1;37;40mJARVIS\u001b[0m: You can apply to the Courts for a copy.\n",
      "......................................................................................\n",
      "......................................................................................\n",
      "\u001b[1;37;40mJARVIS\u001b[0m: Good day, How may i of help?\n",
      "......................................................................................\n",
      "......................................................................................\n",
      "\u001b[1;37;40mJARVIS\u001b[0m: This depends on the lawyer that you approach.\n",
      "......................................................................................\n",
      "......................................................................................\n",
      "\u001b[1;37;40mJARVIS\u001b[0m: Apology, I do not understand. Can you rephrase?\n",
      "......................................................................................\n",
      "......................................................................................\n",
      "\u001b[1;37;40mJARVIS\u001b[0m: In order to adopt a child in Singapore, you must fulfil certain guidelines laid down by the Ministry of Social and Family Development. These can be found here: http://app.customerfeedback.mcys.gov.sg/fcd_faqmain.asp?strFAQSysid=2004919155835&strItemChoice=200499134018&strSubItemChoice=2004919114225&action=SHOWTOPICS&m_strTopicSysID=2004919114242 Essentially, the requirements are that you must be a resident of Singapore, be at least 25 years of age and be at least 21 years older than the child. There are other requirements which may found at the link above.\n",
      "......................................................................................\n",
      "......................................................................................\n",
      "\u001b[1;37;40mJARVIS\u001b[0m: Maintenance is financial support by your former spouse.\n",
      "......................................................................................\n",
      "......................................................................................\n",
      "\u001b[1;37;40mJARVIS\u001b[0m: Apology, I do not understand. Can you rephrase?\n",
      "......................................................................................\n",
      "......................................................................................\n",
      "\u001b[1;37;40mJARVIS\u001b[0m: Apology, I do not understand. Can you rephrase?\n",
      "......................................................................................\n",
      "......................................................................................\n",
      "\u001b[1;37;40mJARVIS\u001b[0m: Bye! Hope that i am of help.\n"
     ]
    }
   ],
   "source": [
    "# flag=True\n",
    "# print(\"......................................................................................\")\n",
    "# print('\\x1b[1;37;40m' + 'Jarvis'+'\\x1b[0m'+': '+ 'My name is Jarvis, a Lawyer Apprentice Bot.')\n",
    "# print('\\x1b[1;37;40m' + 'Jarvis'+'\\x1b[0m'+': '+ 'I will try my best to answer your query.')\n",
    "# print('\\x1b[1;37;40m' + 'Jarvis'+'\\x1b[0m'+': '+ 'If you want to exit, you can type < bye >.')\n",
    "# while(flag==True):\n",
    "#     print(\"......................................................................................\")\n",
    "#     sentence = input('\\x1b[0;30;47m' +\"USER  \"+'\\x1b[0m'+\":\")\n",
    "#     print(\"......................................................................................\")\n",
    "#     if(sentence.lower()!='bye'):\n",
    "#         if(greeting(sentence.lower())!=None):\n",
    "#             print('\\x1b[1;37;40m' + 'JARVIS'+'\\x1b[0m'+': '+ greeting(sentence.lower()))\n",
    "#         else:\n",
    "#             response_primary, response_message, line_id_primary = Talk_To_Javris(sentence)\n",
    "#             print('\\x1b[1;37;40m' + 'JARVIS'+'\\x1b[0m'+': '+response_primary)\n",
    "            \n",
    "#             #For Tracing, comment to remove from print \n",
    "#             #print(\"\")\n",
    "#             #print(\"SCORE: \"+str(line_id_primary))\n",
    "#             #print(\"COR_QUES:\"+response_message)\n",
    "#             #print(\"\")\n",
    "#     else:\n",
    "#         flag=False\n",
    "# print('\\x1b[1;37;40m' + 'JARVIS'+'\\x1b[0m'+': '+\"Bye! Hope that i am of help.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7872\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import nltk\n",
    "import pickle\n",
    "import string\n",
    "import json\n",
    "import timeit\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Initialization code and function definitions go here\n",
    "# ...\n",
    "\n",
    "def chatbot_interface(input_text, history=[]):\n",
    "    if input_text.lower() == 'bye':\n",
    "        history.append(\"User: \" + input_text)\n",
    "        history.append(\"Jarvis: Bye! Hope that I am of help.\")\n",
    "        return history, history\n",
    "\n",
    "    response = greeting(input_text.lower())\n",
    "    if response is not None:\n",
    "        history.append(\"User: \" + input_text)\n",
    "        history.append(\"Jarvis: \" + response)\n",
    "        return history, history\n",
    "    else:\n",
    "        response_primary, response_message, line_id_primary = Talk_To_Javris(input_text)\n",
    "        history.append(\"User: \" + input_text)\n",
    "        history.append(\"Jarvis: \" + response_primary)\n",
    "        return history, history\n",
    "\n",
    "# Load your necessary data and models here\n",
    "# ...\n",
    "\n",
    "# Create the Gradio interface\n",
    "interface = gr.Blocks()\n",
    "\n",
    "with interface:\n",
    "    gr.Markdown(\"# Jarvis - Lawyer Apprentice Bot\")\n",
    "    gr.Markdown(\"I am Jarvis, a Lawyer Apprentice Bot. How can I help you today?\")\n",
    "    \n",
    "    chatbot = gr.Chatbot()\n",
    "    state = gr.State([])  # Initialize the state with an empty list\n",
    "    \n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Enter your message here...\")\n",
    "        btn = gr.Button(\"Submit\")\n",
    "    \n",
    "    def respond(input_text, history):\n",
    "        history, output = chatbot_interface(input_text, history)\n",
    "        messages = [(history[i], history[i+1]) for i in range(0, len(history), 2)]\n",
    "        txt.value = \"\"  # Clear the text input box after submission\n",
    "        return messages, history\n",
    "\n",
    "    txt.submit(respond, [txt, state], [chatbot, state])\n",
    "    btn.click(respond, [txt, state], [chatbot, state])\n",
    "\n",
    "    with gr.Row():\n",
    "        clear = gr.Button(\"Clear Chat\")\n",
    "        clear.click(lambda: ([], []), None, [chatbot, state])\n",
    "\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
